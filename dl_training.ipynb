{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMaiozdlSTLG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/MyDrive/')\n",
        "data_path = './MyDrive/MyDrive/thesis/data/'"
      ],
      "metadata": {
        "id": "_9YqKaBfzTin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24d6bad4-92ef-4bf6-ceb1-68a41d078130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/MyDrive/; to attempt to forcibly remount, call drive.mount(\"/content/MyDrive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "id": "5bgi3KMMHRbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#files = [ file for file in os.listdir(data_path)[:-1] if file.split('_')[3] == \"10000.csv\" ]\n",
        "f = 'punch_norm_2000_10000.csv'\n",
        "#f = 'punch_norm_raw_raw.csv'\n",
        "\n",
        "#data\n",
        "with open(os.path.join(data_path, f)) as fp:\n",
        "  df = pd.read_csv(fp)\n",
        "\n",
        "\n",
        "try:\n",
        "  df = df.drop(['Unnamed: 0'], axis=1)\n",
        "except:\n",
        "  pass\n",
        "print(df.head())\n",
        "\n",
        "try:\n",
        "  period_length = int( f.split('_')[2] )\n",
        "  sampling_rate = int( f.split('_')[3].split('.')[0] )\n",
        "  n_observations = period_length / (sampling_rate / 1000) + 1  #timestamp zero, therefore +1 * 3 axis\n",
        "\n",
        "except:\n",
        "  period_length = 'raw'\n",
        "  sampling_rate = 'raw'\n",
        "\n",
        "n_channels = 3\n",
        "n_punches = max(df['id'])+1\n",
        "max_length = max( (df[df['id']==idx].shape[0])*3 for idx in np.arange(n_punches))\n",
        "\n",
        "print(period_length,\", \", sampling_rate, \"\\n\")\n",
        "\n",
        "#Labels\n",
        "with open(os.path.join(data_path, \"labels.csv\")) as fp:\n",
        "  labels = pd.read_csv(fp)\n",
        "\n",
        "\n",
        "labels.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "FaOLTnHd7hrn",
        "outputId": "83c1c845-b467-4b7b-a4b7-3493d3e97f81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          x         y         z  timestamp  id\n",
            "0  6.793223 -2.383251  2.962314          0   0\n",
            "1  6.317743 -3.380427  3.761405      10000   0\n",
            "2  6.333654 -3.077143  3.530405      20000   0\n",
            "3  5.765556 -3.123323  3.622862      30000   0\n",
            "4  5.353406 -3.081780  3.578873      40000   0\n",
            "2000 ,  10000 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       label\n",
              "0  upper-cut\n",
              "1  upper-cut\n",
              "2  upper-cut\n",
              "3  upper-cut\n",
              "4    frontal"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-745a39b6-c925-46ed-84fe-f5966d33601d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>upper-cut</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>upper-cut</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>upper-cut</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>upper-cut</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>frontal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-745a39b6-c925-46ed-84fe-f5966d33601d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-745a39b6-c925-46ed-84fe-f5966d33601d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-745a39b6-c925-46ed-84fe-f5966d33601d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert df[df['id']==0].shape[0] == n_observations"
      ],
      "metadata": {
        "id": "WgDzxUjy4qPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-processing\n",
        "1. Integer encoding of labels\n",
        "2. Creating torch Dataset object + one-hot encoding\n",
        "3. Apply torch DataLoader with shuffle"
      ],
      "metadata": {
        "id": "CWtSQ5T5H_QG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#==============================================\n",
        "#     1.\n",
        "#==============================================\n",
        "\n",
        "unpack_string = lambda df: df[:1].values[0]\n",
        "\n",
        "def encode_label(label):\n",
        "  if unpack_string(label)=='frontal':\n",
        "    return 0\n",
        "  if unpack_string(label)=='hook':\n",
        "    return 1\n",
        "  if unpack_string(label)=='upper-cut':\n",
        "    return 2\n",
        "  if unpack_string(label)=='no-action':\n",
        "    return 3\n",
        "\n",
        "\n",
        "encoded_labels = labels.apply(encode_label, axis=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "VeHhfjwLIQCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Oc5v34qq47ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#==============================================\n",
        "#     2.\n",
        "#==============================================\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.functional import one_hot\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "class TwoDimPunchDataset(Dataset):\n",
        "     def __init__(self, data, labels, train_indices, max_length):\n",
        "         super(Dataset, self).__init__()\n",
        "\n",
        "         self.data = data                               #to pandas DataFrame\n",
        "         self.labels = torch.from_numpy(labels.values)  #to 1D tensor\n",
        "\n",
        "         #Fit scaler on training data\n",
        "         self.scaler = MinMaxScaler().fit( list( [self.__getitem__(idx, scale=False)[0] for idx in train_indices ] ) )\n",
        "\n",
        "     def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "     def __getitem__(self, index, scale=True):\n",
        "        punch = self.data[self.data['id']==index]\n",
        "\n",
        "        #concatenate all signals to form 1D numpy array\n",
        "        signal = np.concatenate((punch['x'].values, punch['y'].values, punch['z'].values))\n",
        "        dif = max_length - len(signal)\n",
        "        #print(signal.shape)\n",
        "\n",
        "        if dif != 0:\n",
        "            signal = np.concatenate((signal, np.zeros(dif)))\n",
        "\n",
        "        if scale==True:\n",
        "            signal = self.scaler.transform(signal.reshape(1,-1))\n",
        "\n",
        "\n",
        "        label = self.labels[index]\n",
        "        label = one_hot(label, num_classes=4)           # One hot encoding of labels\n",
        "        return signal, label"
      ],
      "metadata": {
        "id": "raATL2Mn0p-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "batch_size = 16\n",
        "random_state = 42\n",
        "n_samples = 7606\n",
        "indices = list(range(n_samples))\n",
        "val_split = int(np.floor(0.1 * n_samples))\n",
        "test_split = int(np.floor(0.2 * n_samples))\n",
        "\n",
        "\n",
        "np.random.seed(random_state)\n",
        "np.random.shuffle(indices)\n",
        "val_indices, test_indices, train_indices =  indices[:val_split], indices[val_split:test_split], indices[test_split:]\n",
        "\n",
        "#Takes a while to fit the scaler vv\n",
        "dataset = TwoDimPunchDataset(df, encoded_labels, train_indices=train_indices, max_length=max_length)"
      ],
      "metadata": {
        "id": "qJNJBHp9q_b6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#==============================================\n",
        "#     3.\n",
        "#==============================================\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "val_sampler = SubsetRandomSampler(val_indices)\n",
        "test_sampler = SubsetRandomSampler(test_indices)\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "val_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
        "test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)"
      ],
      "metadata": {
        "id": "VTaeaAAXzSov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")"
      ],
      "metadata": {
        "id": "Ozt5ed334n28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea7d66f1-a5b8-4e79-87ca-ac13886427ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "def training(model, train_data, n_epochs, lr, criterion, optimizer, patience=10, path=\"./MyDrive/MyDrive/thesis/models/model1/\"):\n",
        "  counter=0\n",
        "  val_losses = []\n",
        "  best_val_loss = 1000\n",
        "  all_losses = {}\n",
        "  if type(n_epochs) == type(int()):\n",
        "    epoch_range = range(1, n_epochs+1)\n",
        "  else:\n",
        "    epoch_range = n_epochs\n",
        "\n",
        "  for epoch in epoch_range:\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for inputs, labels in train_data:\n",
        "      inputs = inputs.float()\n",
        "      #print(f\"input size before padding: {inputs.size()}, \")\n",
        "      #inputs = pad_sequence(inputs)\n",
        "      #print(f\"input size after padding: {inputs.size()}\\n\")\n",
        "\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      model.zero_grad()\n",
        "\n",
        "      output = model(inputs)\n",
        "      pred_shape = output.shape\n",
        "\n",
        "      loss = criterion(output.squeeze(), labels.float())   #squeeze()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for val_inputs, val_labels in val_loader:\n",
        "        val_inputs = val_inputs.float()\n",
        "        val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
        "        y_pred = model(val_inputs)\n",
        "        val_loss = criterion(y_pred.squeeze(), val_labels.float())   #squeeze()\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "    mean_val_loss = torch.mean(torch.Tensor(val_losses))\n",
        "    print(\"Epoch %d: train loss %.4f, validation loss %.4f\" % (epoch, loss, mean_val_loss))\n",
        "    all_losses[epoch] = (int(loss.cpu()), int(mean_val_loss.cpu()))\n",
        "\n",
        "    #save checkpoint\n",
        "    checkpoint_path = os.path.join(path, f\"check_epoch_{epoch}.pt\")\n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            #'model_state_dict': model.state_dict(),\n",
        "            #'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'train loss': loss,\n",
        "            'val loss': mean_val_loss\n",
        "            }, checkpoint_path)\n",
        "\n",
        "\n",
        "    if mean_val_loss < best_val_loss:\n",
        "      best_val_loss = mean_val_loss\n",
        "      counter=0\n",
        "\n",
        "    elif counter == patience:\n",
        "      break\n",
        "\n",
        "    else:\n",
        "      counter+=1\n",
        "\n",
        "  #Epoch loop\n",
        "  #=============================\n",
        "\n",
        "\n",
        "  try:\n",
        "    with open(os.path.join(path,'losses.json'), 'w') as f:\n",
        "      json.dump(all_losses, f)\n",
        "  except:\n",
        "    print(\"Could not save losses.\")\n",
        "\n",
        "  return model\n",
        "\n",
        "class LSTM_model(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, num_layers=1, output_dim=4):\n",
        "    super().__init__()\n",
        "    self.recurrent = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "    self.dropout = nn.Dropout(0.01)\n",
        "    self.linear1 = nn.Linear(hidden_dim, hidden_dim//2)\n",
        "    self.linear2 = nn.Linear(hidden_dim//2, output_dim)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.out_layer = nn.Softmax(dim=2)\n",
        "\n",
        "  def forward(self, input_tensor):\n",
        "    #input_tensor = pad_sequence(input_tensor)\n",
        "    x, h_n= self.recurrent(input_tensor)\n",
        "    x = self.dropout(x)\n",
        "    x = self.linear1(x)\n",
        "\n",
        "    x = self.relu(x)\n",
        "    output = self.linear2(x)\n",
        "    #print(output)\n",
        "    output = self.out_layer(output)\n",
        "    return output\n",
        "\n",
        "class CONV_model(nn.Module):\n",
        "  def __init__(self, num_classes=4):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv1d(in_channels = 1, out_channels = 8, kernel_size = 5, padding=3)\n",
        "    self.maxp1 = nn.MaxPool1d(2)\n",
        "    self.bn1 = nn.BatchNorm1d(num_features = 8)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.conv2 = nn.Conv1d(in_channels = 8, out_channels = 16, kernel_size = 5, padding=3)\n",
        "    self.maxp2 = nn.MaxPool1d(2)\n",
        "    self.bn2 = nn.BatchNorm1d(num_features = 16)\n",
        "\n",
        "    self.conv3 = nn.Conv1d(in_channels = 16, out_channels = 32, kernel_size = 7, padding=4)\n",
        "    self.maxp3 = nn.MaxPool1d(4)\n",
        "    self.bn3 = nn.BatchNorm1d(num_features = 32)\n",
        "\n",
        "\n",
        "    c_ = 47\n",
        "    self.c_=c_\n",
        "    self.fc1 = nn.Linear(in_features = 32*c_, out_features = 8*c_)\n",
        "    self.dropout1 = nn.Dropout(0.01)\n",
        "    self.fc2 = nn.Linear(in_features = 8*c_, out_features = 1*c_)\n",
        "    self.out_layer = nn.Linear(in_features = 1*c_, out_features = num_classes)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, input_tensor):\n",
        "    batch_size = input_tensor.shape[0]\n",
        "\n",
        "    x = self.conv1(input_tensor)\n",
        "    x = self.maxp1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.bn1(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = self.maxp2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.bn2(x)\n",
        "\n",
        "    x = self.conv3(x)\n",
        "    x = self.maxp3(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.bn3(x)\n",
        "\n",
        "    x = x.view(batch_size,32*self.c_)\n",
        "\n",
        "    x = self.dropout1(x)\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout1(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout1(x)\n",
        "\n",
        "    output = self.out_layer(x)\n",
        "    output = self.softmax(output)\n",
        "    return output\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QtQ5QvznypoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#==============================\n",
        "#\n",
        "# Initialize New Model\n",
        "#\n",
        "#==============================\n",
        "\n",
        "\n",
        "model_name = f\"cnn_{period_length}_{sampling_rate}\"\n",
        "#model_name = \"temp\"\n",
        "model_path = f\"./MyDrive/MyDrive/thesis/models/{model_name}/\"\n",
        "try:\n",
        "  os.mkdir(model_path)\n",
        "except:\n",
        "  print(f\"Model path: {model_path} already exists.\")\n",
        "\n",
        "\"\"\"\n",
        "#RNN\n",
        "if period_length=='raw':\n",
        "  model = LSTM_model(max_length, 128, num_layers=1, output_dim=4)\n",
        "else:\n",
        "  model = LSTM_model(int(n_observations*3), 128, num_layers=1, output_dim=4)\n",
        "\"\"\"\n",
        "#CNN\n",
        "if period_length=='raw':\n",
        "  model = CONV_model(num_classes=4)\n",
        "else:\n",
        "  model = CONV_model(num_classes=4)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "n_epochs = 100\n",
        "learning_rate = 0.0001\n",
        "decay = 0.0001\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=decay)\n",
        "\n",
        "patience = 5"
      ],
      "metadata": {
        "id": "D4L3hF08v77w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5fa6485-2073-401d-cbd6-5bfc4b879254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model path: ./MyDrive/MyDrive/thesis/models/cnn_2500_10000/ already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#==============================\n",
        "#\n",
        "# Load Model, resume training\n",
        "#\n",
        "#==============================\n",
        "\n",
        "\n",
        "model_name = f\"rnn_{period_length}_{sampling_rate}\"\n",
        "model_path = f\"./MyDrive/MyDrive/thesis/models/{model_name}/\"\n",
        "\n",
        "checkpoint = os.listdir(model_path)[-1]\n",
        "checkpoint_path = os.path.join(model_path, checkpoint)\n",
        "try:\n",
        "  os.mkdir(model_path)\n",
        "except:\n",
        "  print(f\"Model path: {model_path} already exists.\")\n",
        "\n",
        "\n",
        "\n",
        "model = LSTM_model(int(n_observations*3), 256,output_dim=4)\n",
        "model.load_state_dict(torch.load(checkpoint_path)['model_state_dict'])\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "n_epochs = range(int( checkpoint.split('_')[2].split('.')[0] ), 100 + 1)\n",
        "learning_rate = 0.001\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "patience = 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "0n8TV-2jluIw",
        "outputId": "a8dca16d-38e4-4dfe-a814-5436476e839a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model path: ./MyDrive/MyDrive/thesis/models/rnn_2000_10000/ already exists.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-52fee9cc8042>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_observations\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2041\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2042\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2043\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LSTM_model:\n\tsize mismatch for recurrent.weight_ih_l0: copying a param with shape torch.Size([512, 603]) from checkpoint, the shape in current model is torch.Size([1024, 603]).\n\tsize mismatch for recurrent.weight_hh_l0: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for recurrent.bias_ih_l0: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for recurrent.bias_hh_l0: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for linear1.weight: copying a param with shape torch.Size([64, 128]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for linear1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for linear2.weight: copying a param with shape torch.Size([4, 64]) from checkpoint, the shape in current model is torch.Size([4, 128])."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mod1= training(model, train_loader, n_epochs, learning_rate, criterion, optimizer, patience=patience, path=model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UN58-LxBaitC",
        "outputId": "ffbbb90d-f9ed-4772-8e0b-46f1310d9008"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train loss 0.8132, validation loss 0.9006\n",
            "Epoch 2: train loss 0.7592, validation loss 0.8695\n",
            "Epoch 3: train loss 1.0276, validation loss 0.8472\n",
            "Epoch 4: train loss 0.7478, validation loss 0.8343\n",
            "Epoch 5: train loss 0.7471, validation loss 0.8239\n",
            "Epoch 6: train loss 0.7455, validation loss 0.8165\n",
            "Epoch 7: train loss 0.7633, validation loss 0.8117\n",
            "Epoch 8: train loss 0.7454, validation loss 0.8076\n",
            "Epoch 9: train loss 0.7548, validation loss 0.8044\n",
            "Epoch 10: train loss 0.7439, validation loss 0.8010\n",
            "Epoch 11: train loss 0.7437, validation loss 0.7979\n",
            "Epoch 12: train loss 0.7658, validation loss 0.7957\n",
            "Epoch 13: train loss 0.7442, validation loss 0.7935\n",
            "Epoch 14: train loss 1.0753, validation loss 0.7916\n",
            "Epoch 15: train loss 0.7439, validation loss 0.7900\n",
            "Epoch 16: train loss 1.0778, validation loss 0.7885\n",
            "Epoch 17: train loss 0.7440, validation loss 0.7874\n",
            "Epoch 18: train loss 0.7437, validation loss 0.7866\n",
            "Epoch 19: train loss 0.7496, validation loss 0.7855\n",
            "Epoch 20: train loss 0.7490, validation loss 0.7845\n",
            "Epoch 21: train loss 0.7437, validation loss 0.7835\n",
            "Epoch 22: train loss 0.7437, validation loss 0.7827\n",
            "Epoch 23: train loss 0.7437, validation loss 0.7819\n",
            "Epoch 24: train loss 0.7438, validation loss 0.7811\n",
            "Epoch 25: train loss 0.7440, validation loss 0.7804\n",
            "Epoch 26: train loss 0.7437, validation loss 0.7800\n",
            "Epoch 27: train loss 0.7474, validation loss 0.7794\n",
            "Epoch 28: train loss 0.7437, validation loss 0.7788\n",
            "Epoch 29: train loss 0.7437, validation loss 0.7784\n",
            "Epoch 30: train loss 0.7450, validation loss 0.7779\n",
            "Epoch 31: train loss 0.7437, validation loss 0.7775\n",
            "Epoch 32: train loss 0.7465, validation loss 0.7770\n",
            "Epoch 33: train loss 0.7625, validation loss 0.7768\n",
            "Epoch 34: train loss 0.7437, validation loss 0.7765\n",
            "Epoch 35: train loss 0.7437, validation loss 0.7761\n",
            "Epoch 36: train loss 0.7444, validation loss 0.7760\n",
            "Epoch 37: train loss 0.8197, validation loss 0.7756\n",
            "Epoch 38: train loss 0.7437, validation loss 0.7752\n",
            "Epoch 39: train loss 0.7437, validation loss 0.7750\n",
            "Epoch 40: train loss 0.7437, validation loss 0.7747\n",
            "Epoch 41: train loss 0.7437, validation loss 0.7745\n",
            "Epoch 42: train loss 0.7438, validation loss 0.7744\n",
            "Epoch 43: train loss 0.7437, validation loss 0.7740\n",
            "Epoch 44: train loss 0.7438, validation loss 0.7738\n",
            "Epoch 45: train loss 0.7437, validation loss 0.7735\n",
            "Epoch 46: train loss 0.7437, validation loss 0.7734\n",
            "Epoch 47: train loss 0.7438, validation loss 0.7732\n",
            "Epoch 48: train loss 0.7437, validation loss 0.7729\n",
            "Epoch 49: train loss 0.7437, validation loss 0.7727\n",
            "Epoch 50: train loss 0.7437, validation loss 0.7727\n",
            "Epoch 51: train loss 0.7440, validation loss 0.7725\n",
            "Epoch 52: train loss 0.7437, validation loss 0.7723\n",
            "Epoch 53: train loss 0.7437, validation loss 0.7721\n",
            "Epoch 54: train loss 0.9310, validation loss 0.7720\n",
            "Epoch 55: train loss 0.7437, validation loss 0.7719\n",
            "Epoch 56: train loss 0.7437, validation loss 0.7720\n",
            "Epoch 57: train loss 0.7442, validation loss 0.7718\n",
            "Epoch 58: train loss 0.7437, validation loss 0.7717\n",
            "Epoch 59: train loss 0.7437, validation loss 0.7715\n",
            "Epoch 60: train loss 0.7437, validation loss 0.7713\n",
            "Epoch 61: train loss 0.7437, validation loss 0.7712\n",
            "Epoch 62: train loss 0.7453, validation loss 0.7711\n",
            "Epoch 63: train loss 0.7438, validation loss 0.7710\n",
            "Epoch 64: train loss 0.7437, validation loss 0.7708\n",
            "Epoch 65: train loss 0.7437, validation loss 0.7707\n",
            "Epoch 66: train loss 0.7527, validation loss 0.7707\n",
            "Epoch 67: train loss 0.7437, validation loss 0.7706\n",
            "Epoch 68: train loss 0.7437, validation loss 0.7705\n",
            "Epoch 69: train loss 0.7437, validation loss 0.7704\n",
            "Epoch 70: train loss 0.7438, validation loss 0.7702\n",
            "Epoch 71: train loss 0.7437, validation loss 0.7702\n",
            "Epoch 72: train loss 0.7439, validation loss 0.7700\n",
            "Epoch 73: train loss 0.7437, validation loss 0.7700\n",
            "Epoch 74: train loss 0.7437, validation loss 0.7699\n",
            "Epoch 75: train loss 0.7437, validation loss 0.7698\n",
            "Epoch 76: train loss 0.7437, validation loss 0.7697\n",
            "Epoch 77: train loss 0.7437, validation loss 0.7696\n",
            "Epoch 78: train loss 0.7454, validation loss 0.7695\n",
            "Epoch 79: train loss 0.7437, validation loss 0.7694\n",
            "Epoch 80: train loss 0.9191, validation loss 0.7693\n",
            "Epoch 81: train loss 0.7437, validation loss 0.7693\n",
            "Epoch 82: train loss 0.7437, validation loss 0.7692\n",
            "Epoch 83: train loss 0.7437, validation loss 0.7691\n",
            "Epoch 84: train loss 0.7437, validation loss 0.7690\n",
            "Epoch 85: train loss 0.7492, validation loss 0.7689\n",
            "Epoch 86: train loss 0.7441, validation loss 0.7689\n",
            "Epoch 87: train loss 0.7437, validation loss 0.7688\n",
            "Epoch 88: train loss 0.7437, validation loss 0.7687\n",
            "Epoch 89: train loss 0.7438, validation loss 0.7686\n",
            "Epoch 90: train loss 0.7437, validation loss 0.7685\n",
            "Epoch 91: train loss 0.7437, validation loss 0.7684\n",
            "Epoch 92: train loss 0.7437, validation loss 0.7684\n",
            "Epoch 93: train loss 0.7437, validation loss 0.7683\n",
            "Epoch 94: train loss 0.7439, validation loss 0.7682\n",
            "Epoch 95: train loss 0.7437, validation loss 0.7681\n",
            "Epoch 96: train loss 0.7437, validation loss 0.7681\n",
            "Epoch 97: train loss 0.8455, validation loss 0.7680\n",
            "Epoch 98: train loss 0.7440, validation loss 0.7679\n",
            "Epoch 99: train loss 0.7445, validation loss 0.7679\n",
            "Epoch 100: train loss 0.7484, validation loss 0.7678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(mod1,f\"./MyDrive/MyDrive/thesis/models/{model_name}/{model_name}.pt\")"
      ],
      "metadata": {
        "id": "m3aOvY6uaiyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#===============================\n",
        "#\n",
        "# Load best state_dict into model\n",
        "#\n",
        "#===============================\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_name = f\"rnn_{period_length}_{sampling_rate}\"\n",
        "model_path = f\"./MyDrive/MyDrive/thesis/models/{model_name}/\"\n",
        "\n",
        "checkpoint_paths = []\n",
        "for checkpoint in os.listdir(model_path)[:-1]:\n",
        "  checkpoint_paths.append(os.path.join(model_path, checkpoint))\n",
        "\n",
        "best_checkpoint_idx = np.argmin(np.array([ torch.load(checkpoint_path)['loss'] for checkpoint_path in checkpoint_paths]))\n",
        "best_checkpoint_path = checkpoint_paths[best_checkpoint_idx]\n",
        "print(best_checkpoint_path)\n",
        "try:\n",
        "  os.mkdir(model_path)\n",
        "except:\n",
        "  print(f\"Model path: {model_path} already exists.\")\n",
        "\n",
        "\n",
        "\n",
        "model = LSTM_model(int(n_observations*3), 256,output_dim=4)\n",
        "model.load_state_dict(torch.load(best_checkpoint_path)['model_state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "qdbOVfunweZ6",
        "outputId": "a6f621ba-8a08-4c5a-c693-3c0e0bb6203e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-3bb2f708ab87>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mcheckpoint_paths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mbest_checkpoint_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcheckpoint_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheckpoint_paths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mbest_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_checkpoint_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-3bb2f708ab87>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mcheckpoint_paths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mbest_checkpoint_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcheckpoint_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheckpoint_paths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mbest_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_checkpoint_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1172\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m             \u001b[0mtyped_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtyped_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0;31m# stop wrapping with TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         typed_storage = torch.storage.TypedStorage(\n\u001b[0;32m-> 1116\u001b[0;31m             \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m             _internal=True)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[1;32m    167\u001b[0m                            \u001b[0;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                            \u001b[0;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install torchmetrics\n",
        "#===============================\n",
        "#\n",
        "# Test model\n",
        "#\n",
        "#===============================\n",
        "from torchmetrics.classification import MulticlassAccuracy\n",
        "from torchmetrics.classification import F1Score\n",
        "\n",
        "accuracy_score = MulticlassAccuracy(num_classes=4).to(device)\n",
        "f1 = F1Score(task='multiclass', num_classes=4, average='macro').to(device)\n",
        "\n",
        "\n",
        "mod1.eval()\n",
        "with torch.no_grad():\n",
        "  count_batches=0\n",
        "  count_accuracy=0\n",
        "  count_f1=0\n",
        "  for inputs, labels in test_loader:\n",
        "    #inputs = inputs[0,:].float()\n",
        "    targets = torch.Tensor( [torch.argmax(label) for label in labels] )\n",
        "    inputs = inputs.float()\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    y_pred = mod1(inputs)\n",
        "    targets = torch.Tensor( [torch.argmax(label) for label in labels] )\n",
        "\n",
        "    batch_accuracy = accuracy_score(y_pred.squeeze().to(device), targets.to(device))\n",
        "    batch_f1 = f1(y_pred.squeeze().to(device), targets.to(device))\n",
        "    count_accuracy+=batch_accuracy\n",
        "    count_f1+=batch_f1\n",
        "    count_batches+=1\n",
        "\n",
        "\n",
        "acc = float((count_accuracy.cpu()/count_batches))\n",
        "f1 = float((count_f1.cpu()/count_batches))\n",
        "print(acc, type(acc))\n",
        "print(f1, type(f1))\n",
        "\n",
        "columns = ['model', 'period length', 'sampling rate', 'accuracy', 'f1', 'learning rate']\n",
        "results = pd.DataFrame([['LSTM',period_length, sampling_rate, acc, f1, learning_rate]],columns = columns)\n",
        "results.to_csv(os.path.join(model_path, f\"{model_name}_results.csv\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQo0HyPh85-L",
        "outputId": "2ef45c06-5dd2-4e44-fa22-098b8ebacfe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9666171073913574 <class 'float'>\n",
            "0.9660329818725586 <class 'float'>\n"
          ]
        }
      ]
    }
  ]
}